{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import jaccard as jac_np\n",
    "from utility import point_form as pf_np\n",
    "from utility import intersect as int_np\n",
    "\n",
    "from box_utils  import jaccard as jac_torch\n",
    "from box_utils  import point_form as pf_torch\n",
    "from box_utils  import intersect  as int_torch \n",
    "\n",
    "from CustomDataLoaderTesting import DataAugmentor\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from SSD_generate_anchors import generate_ssd_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSDBoxSizes = collections.namedtuple('SSDBoxSizes', ['min', 'max'])\n",
    "\n",
    "Spec = collections.namedtuple('Spec', ['feature_map_size', 'shrinkage', 'box_sizes', \n",
    "                                       'aspect_ratios'])\n",
    "\n",
    "# the SSD orignal specs\n",
    "specs = [\n",
    "    Spec(38, 8, SSDBoxSizes(30, 60), [2]),\n",
    "    Spec(19, 16, SSDBoxSizes(60, 111), [2, 3]),\n",
    "    Spec(10, 32, SSDBoxSizes(111, 162), [2, 3]),\n",
    "    Spec(5, 64, SSDBoxSizes(162, 213), [2, 3]),\n",
    "    Spec(3, 100, SSDBoxSizes(213, 264), [2]),\n",
    "    Spec(1, 300, SSDBoxSizes(264, 315), [2])\n",
    "]\n",
    "\n",
    "priors = generate_ssd_priors(specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "root   = Path.home()/'Documents'/'DATASETS'/'VOCdevkit'/'VOC2007'\n",
    "voc_image_path      = root/'JPEGImages'\n",
    "voc_annotation_path = root/'Annotations'\n",
    "voc_trainval_path   = root/'ImageSets'/'Main'/'train.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = DataAugmentor()\n",
    "generator = tester.flow_from_directory(root        = root,\n",
    "                                       data_file   = voc_trainval_path,\n",
    "                                       target_size = 300,\n",
    "                                       batch_size  = 1,\n",
    "                                       shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , target = generator[0]\n",
    "\n",
    "conf_data = target[0][:,0]\n",
    "loc_data  = target[0][:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.305      0.568      0.624      0.542     ]\n",
      " [0.732      0.53066665 0.26133335 0.404     ]]\n",
      "[[-0.007       0.29700002  0.617       0.839     ]\n",
      " [ 0.6013333   0.32866663  0.86266667  0.7326667 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loc_data_np = pf_np(loc_data)                    # point form numpy \n",
    "priors_np   = pf_np(priors).astype(np.float32)   # point form numpy\n",
    "\n",
    "print(loc_data)\n",
    "print(loc_data_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3050, 0.5680, 0.6240, 0.5420],\n",
      "        [0.7320, 0.5307, 0.2613, 0.4040]])\n",
      "tensor([[-0.0070,  0.2970,  0.6170,  0.8390],\n",
      "        [ 0.6013,  0.3287,  0.8627,  0.7327]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loc_data_th = torch.from_numpy(loc_data)\n",
    "\n",
    "priors_th   = torch.from_numpy(priors_np).float()\n",
    "\n",
    "loc_data_th_pf = pf_torch(loc_data_th) # point from in torch\n",
    "# priors_th_pf   = pf_torch(priors_th)   # point form in torch\n",
    "\n",
    "print(loc_data_th)\n",
    "print(loc_data_th_pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_data_th.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_np = jac_np(box_a = loc_data_np, box_b = priors_np)\n",
    "iou_th = jac_torch(box_a = loc_data_th_pf, box_b= priors_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (iou_np == iou_th.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17464"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_np(matched = None, priors = None, variances = [0.1, 0.2]):\n",
    "    '''\n",
    "    Encode the variance from the priorbox layers inot the ground truth boxes \n",
    "    we have macthed  (based on jaccard overlap) with the prior boxes\n",
    "    Args:\n",
    "        matched:  (tensor) coords of ground truth for each prior in point_form \n",
    "                   shape = [num_priors, 4]\n",
    "       priors  : (tensor) priors boxes in center-offset form \n",
    "                   shape = [num_priors, 4]\n",
    "       variance: list(float) Variance of prior boxes\n",
    "\n",
    "    Returns:\n",
    "        encoded boxes: (tensor) shape = [num_priors, 4]\n",
    "    '''\n",
    "    g_cxcy = (matched[:,:2] + matched[:,:2])/2 - priors[:,:2]\n",
    "    \n",
    "    # encoded variance\n",
    "    g_cxcy /= (variances[0] * priors[:,2:])\n",
    "    \n",
    "    # match width/height with priors width and height\n",
    "    g_wh  = (matched[:,2:] - matched[:,:2]) / priors[:,2:]\n",
    "    g_wh  = np.log(g_wh) / variances[1]\n",
    "    \n",
    "    return np.concatenate((g_cxcy, g_wh), axis = -1)\n",
    "\n",
    "def encode_torch(matched, priors, variances):\n",
    "    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n",
    "    we have matched (based on jaccard overlap) with the prior boxes.\n",
    "    Args:\n",
    "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
    "            Shape: [num_priors, 4].\n",
    "        priors: (tensor) Prior boxes in center-offset form\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        encoded boxes (tensor), Shape: [num_priors, 4]\n",
    "    \"\"\"\n",
    "\n",
    "    # dist b/t match center and prior's center\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n",
    "    \n",
    "    # encode variance\n",
    "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
    "    \n",
    "    # match wh / prior wh\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
    "    g_wh = torch.log(g_wh) / variances[1]\n",
    "    \n",
    "    # return target for smooth_l1_loss\n",
    "    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_argmax(a, axis = 0):\n",
    "    \n",
    "    if not axis == 0:\n",
    "        raise ValueError('to be used only for axis 0')\n",
    "    \n",
    "    row, col = a.shape[:2]\n",
    "    output = []\n",
    "    for i in range(col):\n",
    "        x = a[:,i]\n",
    "        index = np.where(x == x.max(axis = 0))[0][-1] # the last entry in the array\n",
    "        output.append(index)\n",
    "        \n",
    "    return np.array(output)\n",
    "\n",
    "def index_fill(array, index, axis, value):\n",
    "    dim = array.shape\n",
    "    \n",
    "    if axis == 1:\n",
    "        for i in range(dim[0]):\n",
    "            np.put(array[i,:], index, value)\n",
    "            \n",
    "    elif axis == 0:\n",
    "        for i in range(dim[1]):\n",
    "            np.put(array[:,i], index, value)\n",
    "            \n",
    "    return array\n",
    "\n",
    "def match_np(truths      = None, \n",
    "          labels     = None, \n",
    "          priors     = None, \n",
    "          variance   = None, \n",
    "          threshold  = 0.5,\n",
    "        ):\n",
    "\n",
    "    \n",
    "    iou = jac_np(truths, priors)\n",
    "    \n",
    "    best_prior_overlap = np.amax(iou, axis=-1).astype(np.float32)\n",
    "    best_prior_idx     = np.argmax(iou, axis =-1)\n",
    "    \n",
    "# #    print(best_prior_overlap.shape)\n",
    "# #    print(best_prior_idx.shape)\n",
    "\n",
    "    best_truth_overlap = np.amax(iou, axis=0).astype(np.float32)\n",
    "    best_truth_idx     = numpy_argmax(iou)\n",
    "    \n",
    "#     best_truth_overlap = index_fill(best_truth_overlap, best_prior_idx, axis=0, value=2)   \n",
    "# #    print(best_truth_overlap.shape)\n",
    "# #    print(best_truth_idx.shape)\n",
    "\n",
    "    for j in range(best_prior_idx.shape[0]):\n",
    "        best_truth_idx[best_prior_idx[j]] = j\n",
    "    \n",
    "    matches = truths[best_truth_idx]\n",
    "    \n",
    "#     conf    = labels[best_truth_idx]\n",
    "#     conf[best_truth_overlap < threshold] = 0\n",
    "    \n",
    "    loc     = encode_np(matched=matches, priors=priors, variances=variance)\n",
    "    \n",
    "    return loc \n",
    "\n",
    "\n",
    "def match_torch(threshold = 0.5, \n",
    "                truths = None, \n",
    "                priors = None, \n",
    "                variances = [0.1, 0.2], \n",
    "                labels = None):\n",
    "    \n",
    "    overlaps = jac_torch(truths, priors)\n",
    "    \n",
    "    # (Bipartite Matching)\n",
    "    # [1,num_objects] best prior for each ground truth\n",
    "    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n",
    "#     # [1,num_priors] best ground truth for each prior\n",
    "    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n",
    "    \n",
    "    best_truth_idx.squeeze_(0)\n",
    "    best_truth_overlap.squeeze_(0)\n",
    "    best_prior_idx.squeeze_(1)\n",
    "    best_prior_overlap.squeeze_(1)\n",
    "    \n",
    "    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n",
    "    \n",
    "    # TODO refactor: index  best_prior_idx with long tensor\n",
    "    # ensure every gt matches with its prior of max overlap\n",
    "    for j in range(best_prior_idx.size(0)):\n",
    "        best_truth_idx[best_prior_idx[j]] = j\n",
    "        \n",
    "    matches = truths[best_truth_idx]          # Shape: [num_priors,4]\n",
    "    \n",
    "#     conf = labels[best_truth_idx] + 1         # Shape: [num_priors]\n",
    "#     conf[best_truth_overlap < threshold] = 0  # label as background\n",
    "    loc = encode_torch(matches, priors, variances)\n",
    "\n",
    "    return  loc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestPO_np  = match_np(truths=loc_data_np, priors=priors_np, threshold=0.5, variance=[0.1,0.2], labels=None)\n",
    "bestPO_th  = match_torch(truths=loc_data_th_pf, priors=priors_th, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 4)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestPO_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8732, 4])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestPO_th.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17464"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = (bestPO_np == bestPO_th.numpy())\n",
    "np.sum(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True,  True],\n",
       "       [False, False,  True,  True],\n",
       "       [False, False,  True,  True],\n",
       "       ...,\n",
       "       [False, False,  True,  True],\n",
       "       [False, False,  True,  True],\n",
       "       [False, False,  True,  True]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
